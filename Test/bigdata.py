import csv
import re
import time
from datetime import datetime, timedelta

# --- Selenium ---
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# --- BeautifulSoup ---
from bs4 import BeautifulSoup

# --- Progress bar ---
from tqdm import tqdm

## ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤‡∏Ç‡∏≠‡∏á Pantip

def parse_relative_time(text: str):
    """
    ‡πÄ‡∏ä‡πà‡∏ô "3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß", "15 ‡∏ô‡∏≤‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß", "2 ‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô"
    ‡∏Ñ‡∏∑‡∏ô datetime.now() - timedelta(...) ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
    ‡∏ñ‡πâ‡∏≤ parse ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ return None
    """
    text = text.replace("\xa0", " ").replace("\r", "").strip()

    unit_map = {
        "‡∏ô‡∏≤‡∏ó‡∏µ": "minutes", "‡∏ô‡∏≤‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß": "minutes",
        "‡∏ä‡∏°.": "hours", "‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á": "hours",
        "‡∏ß‡∏±‡∏ô": "days",
        "‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ": "seconds", "‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß": "seconds"
    }
    pattern = r"(\d+)\s*(‡∏ô‡∏≤‡∏ó‡∏µ|‡∏ä‡∏°\.|‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á|‡∏ß‡∏±‡∏ô|‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)(‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß|‡∏Å‡πà‡∏≠‡∏ô)?"
    m = re.search(pattern, text)
    if not m:
        return None

    try:
        amount = int(m.group(1))
        if amount > 999999:
            return None
    except:
        return None

    now = datetime.now()
    unit_thai = m.group(2)
    time_unit = unit_map.get(unit_thai)
    if not time_unit:
        return None

    return now - timedelta(**{time_unit: amount})


def parse_full_thai_datetime(text: str):
    """
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö:
      - "5 ‡πÄ‡∏°.‡∏¢." / "5 ‡πÄ‡∏°‡∏¢" / "6 ‡πÄ‡∏°.‡∏¢"
      - "6 ‡πÄ‡∏°.‡∏¢. 2566", "6 ‡πÄ‡∏°.‡∏¢. 66"
      - ‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô "‡πÄ‡∏ß‡∏•‡∏≤ 12:30 ‡∏ô."
    ‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ return None
    """
    text = text.replace("\xa0"," ").replace("\r","").strip()

    # normalize ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
    def normalize_month(m_str: str):
        alias = {
            "‡πÄ‡∏°‡∏¢": "‡πÄ‡∏°.‡∏¢.",
            "‡πÄ‡∏°‡∏¢.": "‡πÄ‡∏°.‡∏¢.",
            "‡πÄ‡∏°.‡∏¢": "‡πÄ‡∏°.‡∏¢.",
            "‡πÄ‡∏°.‡∏¢.": "‡πÄ‡∏°.‡∏¢."
        }
        return alias.get(m_str, m_str)

    # dict ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÑ‡∏ó‡∏¢ -> ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    thai_months = {
        "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°": 1, "‡∏°.‡∏Ñ.": 1,
        "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå": 2, "‡∏Å.‡∏û.": 2,
        "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°": 3, "‡∏°‡∏µ.‡∏Ñ.": 3,
        "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô": 4, "‡πÄ‡∏°.‡∏¢.": 4,
        "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°": 5, "‡∏û.‡∏Ñ.": 5,
        "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô": 6, "‡∏°‡∏¥.‡∏¢.": 6,
        "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°": 7, "‡∏Å.‡∏Ñ.": 7,
        "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°": 8, "‡∏™.‡∏Ñ.": 8,
        "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô": 9, "‡∏Å.‡∏¢.": 9,
        "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°": 10, "‡∏ï.‡∏Ñ.": 10,
        "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô": 11, "‡∏û.‡∏¢.": 11,
        "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°": 12, "‡∏ò.‡∏Ñ.": 12
    }

    # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏õ‡∏µ
    pattern_full = r"(\d{1,2})\s+([‡∏Å-‡∏Æ.]+)\s+(\d{2,4})\s*(?:‡πÄ‡∏ß‡∏•‡∏≤\s*(\d{1,2}):(\d{1,2})\s*‡∏ô\.)?"
    m = re.search(pattern_full, text)
    if m:
        try:
            day_str = m.group(1)
            raw_month = normalize_month(m.group(2))
            year_str = m.group(3)
            hour_str = m.group(4)
            minute_str = m.group(5)

            day = int(day_str)
            month = thai_months.get(raw_month)
            if not month:
                return None

            if len(year_str) == 2:
                year_thai = 2500 + int(year_str)
            else:
                year_thai = int(year_str)

            year = year_thai - 543
            hour = int(hour_str) if hour_str else 0
            minute = int(minute_str) if minute_str else 0
            return datetime(year, month, day, hour, minute)
        except:
            return None

    # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏µ => "5 ‡πÄ‡∏°.‡∏¢."
    pattern_no_year = r"(\d{1,2})\s+([‡∏Å-‡∏Æ.]+)"
    m = re.search(pattern_no_year, text)
    if m:
        try:
            day_str = m.group(1)
            raw_month = normalize_month(m.group(2))
            month = thai_months.get(raw_month)
            if not month:
                return None
            now = datetime.now()
            year = now.year
            hour = now.hour
            minute = now.minute
            return datetime(year, month, int(day_str), hour, minute)
        except:
            return None

    return None


def parse_thai_datetime(text: str):
    """
    ‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£ parse ‡∏ó‡∏±‡πâ‡∏á relative time ‡πÅ‡∏•‡∏∞ full Thai datetime
    """
    if not text:
        return None

    # ‡∏•‡∏≠‡∏á‡πÅ‡∏ö‡∏ö relative ‡∏Å‡πà‡∏≠‡∏ô
    dt_rel = parse_relative_time(text)
    if dt_rel:
        return dt_rel

    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà match, ‡∏•‡∏≠‡∏á parse ‡πÄ‡∏ï‡πá‡∏°
    dt_full = parse_full_thai_datetime(text)
    if dt_full:
        return dt_full

    return None


def format_datetime(dt: datetime):
    """‡πÅ‡∏õ‡∏•‡∏á datetime ‡πÄ‡∏õ‡πá‡∏ô string"""
    if not dt:
        return "N/A"
    return dt.strftime("%Y-%m-%d %H:%M:%S")


############################################################################
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô scroll ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ
############################################################################

def scroll_in_post(driver, scroll_times=10, wait_sec=1.5):
    """
    ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ Pantip ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö
    """
    last_h = driver.execute_script("return document.body.scrollHeight")
    for _ in range(scroll_times):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(wait_sec)
        new_h = driver.execute_script("return document.body.scrollHeight")
        if new_h == last_h:
            # ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡∏Ç‡∏¢‡∏±‡∏ö ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°
            break
        last_h = new_h


############################################################################
# MAIN SCRIPT
############################################################################

def main():
    # --- CONFIG ---
    TAG_URL = "https://pantip.com/tag/%E0%B8%AB%E0%B8%B8%E0%B9%89%E0%B8%99"
    MAX_SCROLLS = 10      # scroll ‡∏´‡∏ô‡πâ‡∏≤ tag ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    SCROLL_WAIT = 2.5
    MAX_DATA = 8000       # ‡πÄ‡∏Å‡πá‡∏ö (post + comment) ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏™‡∏±‡∏Å 6000

    POST_SCROLL_TIMES = 10
    POST_SCROLL_WAIT = 1.5

    OUTPUT_CSV = "pantip_data_8000.csv"

    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Selenium
    chrome_options = Options()
    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
    chrome_options.add_argument('--log-level=3')
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    try:
        # ‡πÄ‡∏Ç‡πâ‡∏≤‡πÅ‡∏ó‡πá‡∏Å "‡∏´‡∏∏‡πâ‡∏ô"
        driver.get(TAG_URL)
        time.sleep(3)

        print(f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (scroll) ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î {MAX_SCROLLS} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡πÇ‡∏û‡∏™‡∏ï‡πå...")
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        all_post_links = []

        # --- Scroll ‡∏´‡∏ô‡πâ‡∏≤ Tag ---
        while scroll_count < MAX_SCROLLS:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(SCROLL_WAIT)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
            scroll_count += 1

            # parse
            soup = BeautifulSoup(driver.page_source, "html.parser")
            container = soup.select_one("div.container div.col-lg-8")
            if container:
                posts = container.select("li.pt-list-item")
                for p in posts:
                    link_tag = p.select_one("div.pt-list-item__title a")
                    if link_tag:
                        href = link_tag.get("href")
                        full_url = href if href.startswith("http") else "https://pantip.com" + href
                        if full_url not in all_post_links:
                            all_post_links.append(full_url)

        print(f"‡πÄ‡∏à‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(all_post_links)}")

        if not all_post_links:
            print("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡πÉ‡∏î ‡πÜ ‡πÄ‡∏•‡∏¢")
            return

        # ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå CSV
        # ‡πÉ‡∏ä‡πâ quoting=csv.QUOTE_ALL ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ CSV column ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô
        with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô header
            writer.writerow(["post_id","type","day","datetime","title","content","author","raw_date"])
            f.flush()

            from tqdm import tqdm
            pbar = tqdm(total=MAX_DATA, desc="Scraping", unit="records")

            total_rows = 0
            try:
                # ‡πÑ‡∏•‡πà‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏û‡∏™‡∏ï‡πå
                for link in all_post_links:
                    if total_rows >= MAX_DATA:
                        break

                    # ‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏û‡∏™‡∏ï‡πå
                    driver.get(link)
                    time.sleep(2)

                    # scroll ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå
                    scroll_in_post(driver, scroll_times=POST_SCROLL_TIMES, wait_sec=POST_SCROLL_WAIT)

                    soup_post = BeautifulSoup(driver.page_source, "html.parser")

                    # --- ‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏´‡∏•‡∏±‡∏Å ---
                    title_tag = soup_post.select_one("title")
                    title = title_tag.get_text(strip=True) if title_tag else "N/A"

                    # ‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ô-‡πÄ‡∏ß‡∏•‡∏≤ ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ô‡πÇ‡∏û‡∏™‡∏ï‡πå
                    post_dt_div = soup_post.select_one("div.display-post-status-leftside")
                    dt_str_raw = post_dt_div.get_text(strip=True) if post_dt_div else ""
                    dt_obj = parse_thai_datetime(dt_str_raw)
                    day_str = dt_obj.strftime("%Y-%m-%d") if dt_obj else "N/A"
                    final_dt_str = format_datetime(dt_obj)

                    author_tag = soup_post.select_one(".display-post-name")
                    author = author_tag.get_text(strip=True) if author_tag else "Unknown"

                    # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á CSV
                    writer.writerow([
                        link,        # post_id
                        "post",      # type
                        day_str,     # day (YYYY-MM-DD)
                        final_dt_str,# datetime
                        title,       # title
                        "‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏´‡∏•‡∏±‡∏Å", # content
                        author,      # author
                        dt_str_raw   # raw_date ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÑ‡∏ß‡πâ
                    ])
                    f.flush()  # flush ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                    total_rows += 1
                    pbar.update(1)

                    if total_rows >= MAX_DATA:
                        break

                    # --- ‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå ---
                    comment_blocks = soup_post.select("div[id^='comment-']")
                    for c in comment_blocks:
                        if total_rows >= MAX_DATA:
                            break

                        # ‡πÄ‡∏ß‡∏•‡∏≤
                        time_div = c.select_one("div.display-post-status-leftside")
                        comment_raw_date = time_div.get_text(strip=True) if time_div else ""
                        c_dt_obj = parse_thai_datetime(comment_raw_date)
                        c_day_str = c_dt_obj.strftime("%Y-%m-%d") if c_dt_obj else "N/A"
                        c_final_dt_str = format_datetime(c_dt_obj)

                        # ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                        content_div = c.select_one("div.display-post-message")
                        comment_text = content_div.get_text(" ", strip=True) if content_div else "N/A"

                        # ‡∏Ñ‡∏ô‡πÇ‡∏û‡∏™‡∏ï‡πå
                        c_author_tag = c.select_one(".display-post-name")
                        c_author = c_author_tag.get_text(strip=True) if c_author_tag else "Unknown"

                        writer.writerow([
                            link,           # post_id
                            "comment",      # type
                            c_day_str,      # day
                            c_final_dt_str, # datetime
                            title,          # title (‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏´‡∏•‡∏±‡∏Å)
                            comment_text,   # content
                            c_author,       # author
                            comment_raw_date # raw_date
                        ])
                        f.flush()
                        total_rows += 1
                        pbar.update(1)

                        if total_rows >= MAX_DATA:
                            break

            except KeyboardInterrupt:
                print("\nüõë ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (KeyboardInterrupt)! ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏•‡πâ‡∏ß")
            finally:
                pbar.close()

            print(f"\nüéâ ‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏ß‡∏° {total_rows} ‡πÅ‡∏ñ‡∏ß ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå '{OUTPUT_CSV}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

    finally:
        driver.quit()
        print("‡∏õ‡∏¥‡∏î‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")


if __name__ == "__main__":
    main()